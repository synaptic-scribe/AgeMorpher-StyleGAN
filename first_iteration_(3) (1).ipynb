{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ce2ea2-fcbb-41ef-ba92-4387cbbcafa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ce2ea2-fcbb-41ef-ba92-4387cbbcafa3",
        "outputId": "b417bdb3-6f46-4b76-f51d-54875687888e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E8tV6ZzC3Dcy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8tV6ZzC3Dcy",
        "outputId": "59c4fa20-98e3-4737-f969-1f24891ed3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b45490-a420-4092-884c-e1180d55cb05",
      "metadata": {
        "id": "18b45490-a420-4092-884c-e1180d55cb05"
      },
      "outputs": [],
      "source": [
        "!pip install wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af2d31f-8c0a-4ac2-95e9-a62c63b293ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2af2d31f-8c0a-4ac2-95e9-a62c63b293ba",
        "outputId": "05e67ad0-b9fa-4426-a4c9-4dc0b635245b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas --target=/srv/jupyter/python-venv/lib/python3.10/site-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b1fda1-a1be-4877-90d7-4f0c0543faeb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27b1fda1-a1be-4877-90d7-4f0c0543faeb",
        "outputId": "4dd39382-9e81-4a6e-8f01-507fde10b4dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==2.2.3\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas==2.2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c97561-68f5-4fd6-b87d-b7d922e49bad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c97561-68f5-4fd6-b87d-b7d922e49bad",
        "outputId": "818b9370-347e-42f1-cf8e-e9eca8ca320e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb9b95f-03c2-4dbf-bb41-1e75419a3dfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cb9b95f-03c2-4dbf-bb41-1e75419a3dfd",
        "outputId": "1dc88ef7-68e8-4008-e0a1-08a72634f6b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pip install glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554247b1-ad09-4539-880a-f292f5311be5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554247b1-ad09-4539-880a-f292f5311be5",
        "outputId": "f53207c9-e0f4-43f3-dc9a-9883ba489f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbbc2f7-3345-4a12-ad42-4e2d4e3d880a",
      "metadata": {
        "id": "3cbbc2f7-3345-4a12-ad42-4e2d4e3d880a"
      },
      "outputs": [],
      "source": [
        "# class MorphDataset:\n",
        "#     def __init__(self, images_dir, csv_path, batch_size=32, image_size=(256, 256)):\n",
        "#         \"\"\"\n",
        "#         Initialize the MorphDataset for age progression GAN.\n",
        "\n",
        "#         Args:\n",
        "#             images_dir: Directory containing the image files\n",
        "#             csv_path: Path to CSV with metadata (age, gender, filename)\n",
        "#             batch_size: Batch size for the dataset\n",
        "#             image_size: Tuple of (height, width) for resizing images\n",
        "#         \"\"\"\n",
        "#         self.images_dir = images_dir\n",
        "#         self.batch_size = batch_size\n",
        "#         self.image_size = image_size\n",
        "\n",
        "#         # Load CSV data\n",
        "#         df = pd.read_csv(csv_path)\n",
        "#         print(f\"CSV loaded with {len(df)} entries\")\n",
        "\n",
        "#         # Extract filenames, ages, and genders\n",
        "#         self.filenames = df['filename'].values\n",
        "#         self.ages = df['age'].values\n",
        "#         self.genders = df['gender'].values\n",
        "\n",
        "#         # Create full paths to images\n",
        "#         self.image_paths = [os.path.join(images_dir, fname) for fname in self.filenames]\n",
        "\n",
        "#         # Verify file existence\n",
        "#         valid_paths = sum(os.path.exists(path) for path in self.image_paths[:100])\n",
        "#         print(f\"Found {valid_paths} valid paths among first 100 checked\")\n",
        "\n",
        "#     def load_images(self):\n",
        "#         \"\"\"Load images from disk and match with metadata from CSV\"\"\"\n",
        "#         print(f\"Loading images from {self.images_dir}\")\n",
        "\n",
        "#         # Get all image files from the directory\n",
        "#         all_img_files = set()\n",
        "#         for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "#             all_img_files.update(os.path.basename(f) for f in glob.glob(os.path.join(self.images_dir, ext)))\n",
        "\n",
        "#         print(f\"Found {len(all_img_files)} image files in directory\")\n",
        "\n",
        "#         # Create a mapping from filename to index in our arrays\n",
        "#         filename_to_index = {fname: i for i, fname in enumerate(self.filenames)}\n",
        "\n",
        "#         # Check how many files from CSV exist in the directory\n",
        "#         found_files = [f for f in self.filenames if f in all_img_files]\n",
        "#         print(f\"Found {len(found_files)} matching files between CSV and directory\")\n",
        "\n",
        "#         # Load images that match with our CSV data\n",
        "#         images = []\n",
        "#         ages = []\n",
        "#         genders = []\n",
        "\n",
        "#         for filename in found_files:\n",
        "#             idx = filename_to_index[filename]\n",
        "#             img_path = os.path.join(self.images_dir, filename)\n",
        "\n",
        "#             try:\n",
        "#                 img = cv2.imread(img_path)\n",
        "#                 if img is not None:\n",
        "#                     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#                     img = cv2.resize(img, self.image_size).astype('float32') / 255.0\n",
        "#                     images.append(img)\n",
        "#                     ages.append(self.ages[idx])\n",
        "#                     genders.append(self.genders[idx])\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "#         print(f\"Successfully loaded {len(images)} images with matching metadata\")\n",
        "\n",
        "#         # Convert to numpy arrays\n",
        "#         images = np.array(images)\n",
        "#         ages = np.array(ages)\n",
        "#         genders = np.array(genders)\n",
        "\n",
        "#         return images, ages, genders\n",
        "\n",
        "#     def preprocess_image(self, image_path):\n",
        "#         image = tf.io.read_file(image_path)\n",
        "#         image = tf.image.decode_jpeg(image, channels=3)\n",
        "#         image = tf.image.resize(image, self.image_size)\n",
        "#         image = (image / 127.5) - 1  # Normalize to [-1, 1]\n",
        "#         return image\n",
        "\n",
        "#     def create_dataset(self):\n",
        "#         image_paths_ds = tf.data.Dataset.from_tensor_slices(self.image_paths)\n",
        "#         ages_ds = tf.data.Dataset.from_tensor_slices(tf.cast(self.ages, tf.float32))\n",
        "\n",
        "#         # Combine datasets and load images\n",
        "#         dataset = tf.data.Dataset.zip((image_paths_ds, ages_ds))\n",
        "#         dataset = dataset.map(lambda x, y: (self.preprocess_image(x), y),\n",
        "#                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "#         # Ensure ages are shaped correctly (batch_size,) rather than (batch_size, 1)\n",
        "#         dataset = dataset.map(lambda x, y: (x, tf.reshape(y, ())))\n",
        "\n",
        "#         # Batch and prefetch\n",
        "#         dataset = dataset.shuffle(buffer_size=len(self.image_paths))\n",
        "#         dataset = dataset.batch(self.batch_size)\n",
        "#         dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#         return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DoZr1MBWL3G2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZr1MBWL3G2",
        "outputId": "2444c922-96ad-44b9-8d77-f00dcb0cc9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QTy6isuML6h3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTy6isuML6h3",
        "outputId": "98b6c121-c83c-4850-da55-25c4923ced2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zg3WPieXMU2I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg3WPieXMU2I",
        "outputId": "c3ccea1d-6b8f-4613-8eed-2ce4b07c0d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting glob2\n",
            "  Downloading glob2-0.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: glob2\n",
            "  Building wheel for glob2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glob2: filename=glob2-0.7-py2.py3-none-any.whl size=9302 sha256=5949c3466f9536148c6b04ed372b7cfad35b1d93f1cdc472c12c299fa4f84a5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/41/06/9f8fddc6eb1d75bde63db7f491311a4ae26905212617e06eb2\n",
            "Successfully built glob2\n",
            "Installing collected packages: glob2\n",
            "Successfully installed glob2-0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d69f7b-64c5-4d8e-8c40-412c3bba1aa1",
      "metadata": {
        "id": "a6d69f7b-64c5-4d8e-8c40-412c3bba1aa1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3626ee14-775a-4cbe-9c82-303b6e16e0a7",
      "metadata": {
        "id": "3626ee14-775a-4cbe-9c82-303b6e16e0a7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "# from tensorflow.keras.applications import ResNet50, ResNet18\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import glob2 as glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1791dff3-c38d-43e9-a3d2-6221ffff49ea",
      "metadata": {
        "id": "1791dff3-c38d-43e9-a3d2-6221ffff49ea"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac7860b0-7d25-4c11-b555-3639faa44b12",
      "metadata": {
        "id": "ac7860b0-7d25-4c11-b555-3639faa44b12"
      },
      "outputs": [],
      "source": [
        "class MorphDataset:\n",
        "    def __init__(self, images_dir, csv_path, batch_size=32, image_size=(128,128), max_samples=10000):\n",
        "        \"\"\"\n",
        "        Initialize the MorphDataset for age progression GAN.\n",
        "\n",
        "        Args:\n",
        "            images_dir: Directory containing the image files\n",
        "            csv_path: Path to CSV with metadata (age, gender, filename)\n",
        "            batch_size: Batch size for the dataset\n",
        "            image_size: Tuple of (height, width) for resizing images\n",
        "            max_samples: Maximum number of samples to use (default: 10000)\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        # Load CSV data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"CSV loaded with {len(df)} entries\")\n",
        "\n",
        "        # Limit to max_samples if specified\n",
        "        if max_samples and max_samples < len(df):\n",
        "            # Stratified sampling by age to maintain distribution\n",
        "            df['age_group'] = pd.cut(df['age'], bins=10)\n",
        "            df = df.groupby('age_group', group_keys=False).apply(\n",
        "                lambda x: x.sample(min(len(x), int(max_samples * len(x) / len(df))))\n",
        "            )\n",
        "            df = df.drop('age_group', axis=1)\n",
        "\n",
        "            # If we still have too many samples, take a random subset\n",
        "            if len(df) > max_samples:\n",
        "                df = df.sample(max_samples)\n",
        "\n",
        "            print(f\"Dataset reduced to {len(df)} entries\")\n",
        "\n",
        "        # Extract filenames, ages, and genders\n",
        "        self.filenames = df['filename'].values\n",
        "        self.ages = df['age'].values\n",
        "        self.genders = df['gender'].values\n",
        "\n",
        "        # Create full paths to images\n",
        "        self.image_paths = [os.path.join(images_dir, fname) for fname in self.filenames]\n",
        "\n",
        "        # Verify file existence\n",
        "        num_to_check = min(100, len(self.image_paths))\n",
        "        valid_paths = sum(os.path.exists(path) for path in self.image_paths[:num_to_check])\n",
        "        print(f\"Found {valid_paths} valid paths among first {num_to_check} checked\")\n",
        "\n",
        "    def load_images(self):\n",
        "        \"\"\"Load images from disk and match with metadata from CSV\"\"\"\n",
        "        print(f\"Loading images from {self.images_dir}\")\n",
        "\n",
        "        # Create a set of valid filenames for faster lookup\n",
        "        valid_filenames = set(self.filenames)\n",
        "\n",
        "        # For faster processing, only load up to max_samples\n",
        "        limit = min(self.max_samples, len(self.filenames)) if self.max_samples else len(self.filenames)\n",
        "\n",
        "        # Create a mapping from filename to index in our arrays\n",
        "        filename_to_index = {fname: i for i, fname in enumerate(self.filenames)}\n",
        "\n",
        "        # Load images that match with our CSV data\n",
        "        images = []\n",
        "        ages = []\n",
        "        genders = []\n",
        "        count = 0\n",
        "\n",
        "        # Process files in batches to show progress\n",
        "        batch_size = 500\n",
        "        num_batches = (limit + batch_size - 1) // batch_size\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(start_idx + batch_size, limit)\n",
        "\n",
        "            print(f\"Processing batch {batch_idx+1}/{num_batches} ({start_idx}-{end_idx})...\")\n",
        "\n",
        "            for i in range(start_idx, end_idx):\n",
        "                filename = self.filenames[i]\n",
        "                img_path = self.image_paths[i]\n",
        "\n",
        "                if not os.path.exists(img_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                        img = cv2.resize(img, self.image_size).astype('float32') / 255.0\n",
        "                        images.append(img)\n",
        "                        ages.append(self.ages[i])\n",
        "                        genders.append(self.genders[i])\n",
        "                        count += 1\n",
        "\n",
        "                        # Print progress\n",
        "                        if count % 100 == 0:\n",
        "                            print(f\"Loaded {count} images...\")\n",
        "\n",
        "                        # Stop if we've reached our limit\n",
        "                        if count >= self.max_samples:\n",
        "                            break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(images)} images with matching metadata\")\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        images = np.array(images)\n",
        "        ages = np.array(ages)\n",
        "        genders = np.array(genders)\n",
        "\n",
        "        return images, ages, genders\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"Load and preprocess a single image efficiently\"\"\"\n",
        "        try:\n",
        "            image = tf.io.read_file(image_path)\n",
        "            image = tf.image.decode_jpeg(image, channels=3)\n",
        "            image = tf.image.resize(image, self.image_size)\n",
        "            image = (image / 127.5) - 1  # Normalize to [-1, 1]\n",
        "            return image, tf.constant(True)  # Success flag\n",
        "        except tf.errors.InvalidArgumentError:\n",
        "            # Return a black image as placeholder for failed loads\n",
        "            return tf.zeros((*self.image_size, 3)), tf.constant(False)\n",
        "\n",
        "    def get_age(self, index):\n",
        "        \"\"\"Get age value for a given index\"\"\"\n",
        "        return self.ages[index]\n",
        "\n",
        "    def create_dataset(self, cache_path=None):\n",
        "        \"\"\"\n",
        "        Create a TensorFlow dataset with efficient loading.\n",
        "\n",
        "        Args:\n",
        "            cache_path: Optional path to cache the dataset to disk\n",
        "        \"\"\"\n",
        "        # Limit dataset to max_samples if specified\n",
        "        if self.max_samples and self.max_samples < len(self.image_paths):\n",
        "            # Create a stratified sample by age ranges to maintain distribution\n",
        "            indices = list(range(len(self.ages)))\n",
        "            # Sort indices by age\n",
        "            indices.sort(key=lambda i: self.ages[i])\n",
        "            # Take a stratified sample\n",
        "            step = max(1, len(indices) // self.max_samples)\n",
        "            sampled_indices = indices[::step][:self.max_samples]\n",
        "\n",
        "            # Create datasets from the sampled indices\n",
        "            image_paths = [self.image_paths[i] for i in sampled_indices]\n",
        "            ages = [self.ages[i] for i in sampled_indices]\n",
        "        else:\n",
        "            image_paths = self.image_paths\n",
        "            ages = self.ages\n",
        "\n",
        "        print(f\"Creating dataset with {len(image_paths)} images\")\n",
        "\n",
        "        # Create tensor slices\n",
        "        image_paths_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "        ages_ds = tf.data.Dataset.from_tensor_slices(tf.cast(ages, tf.float32))\n",
        "\n",
        "        # Combine datasets\n",
        "        dataset = tf.data.Dataset.zip((image_paths_ds, ages_ds))\n",
        "\n",
        "        # Load and preprocess images with error handling\n",
        "        dataset = dataset.map(\n",
        "            lambda path, age: (\n",
        "                *self.preprocess_image(path),  # Returns (image, success_flag)\n",
        "                age\n",
        "            ),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Filter out failed images\n",
        "        dataset = dataset.filter(lambda img, success, age: success)\n",
        "\n",
        "        # Remove the success flag from the dataset\n",
        "        dataset = dataset.map(lambda img, success, age: (img, age))\n",
        "\n",
        "        # Reshape ages if needed\n",
        "        dataset = dataset.map(lambda x, y: (x, tf.reshape(y, ())))\n",
        "\n",
        "        # Use a smaller shuffle buffer to reduce memory usage\n",
        "        shuffle_buffer_size = min(10000, len(image_paths))\n",
        "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "        # Cache the dataset if a path is provided\n",
        "        if cache_path:\n",
        "            print(f\"Caching dataset to {cache_path}\")\n",
        "            dataset = dataset.cache(cache_path)\n",
        "\n",
        "        # Batch and prefetch\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KbIolTjpynCb",
      "metadata": {
        "id": "KbIolTjpynCb"
      },
      "outputs": [],
      "source": [
        "def build_age_encoder(latent_dim=512):\n",
        "    inputs = layers.Input(shape=(128,128, 3))\n",
        "\n",
        "    # First conv block\n",
        "    x = layers.Conv2D(32, (7, 7), strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # 64x64\n",
        "\n",
        "    # Second conv block\n",
        "    x = layers.Conv2D(64, (5, 5), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # 32x32\n",
        "\n",
        "    # Third conv block\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # 16x16\n",
        "\n",
        "    # Fourth conv block\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # 8x8\n",
        "\n",
        "    # Global pooling and dense layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"age_encoder\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77437d4d-6256-4a47-b1c0-a542052b39f5",
      "metadata": {
        "id": "77437d4d-6256-4a47-b1c0-a542052b39f5"
      },
      "outputs": [],
      "source": [
        "def build_age_classifier():\n",
        "    inputs = layers.Input(shape=(128,128, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding='same')(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 3, padding='same')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 3, padding='same')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dense(1)(x)  # Age output\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"age_classifier\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110e5924-2ff2-431e-bd1f-a7b2c263ab5c",
      "metadata": {
        "id": "110e5924-2ff2-431e-bd1f-a7b2c263ab5c"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(128,128, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(1, 4, strides=1, padding='valid')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"discriminator\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a62638f-6413-4c7f-ba72-91e052598ddf",
      "metadata": {
        "id": "2a62638f-6413-4c7f-ba72-91e052598ddf"
      },
      "outputs": [],
      "source": [
        "def build_style_mapping(latent_dim=512, style_dim=512):\n",
        "    inputs = layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    x = layers.Dense(512)(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dense(512)(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dense(style_dim)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"style_mapping\")\n",
        "    return model\n",
        "\n",
        "def build_age_embedding(style_dim=512):\n",
        "    inputs = layers.Input(shape=(1,))\n",
        "\n",
        "    x = layers.Dense(64)(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dense(style_dim)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"age_embedding\")\n",
        "    return model\n",
        "\n",
        "def build_synthesis_network(style_dim=512):\n",
        "    inputs = layers.Input(shape=(1, 1, style_dim))\n",
        "\n",
        "    # 1x1 -> 4x4\n",
        "    # x = layers.Conv2DTranspose(512, 4, strides=1, padding='valid')(inputs)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 4x4 -> 8x8\n",
        "    # x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 8x8 -> 16x16\n",
        "    # x = layers.Conv2DTranspose(128, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 16x16 -> 32x32\n",
        "    # x = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 32x32 -> 64x64\n",
        "    # x = layers.Conv2DTranspose(32, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 64x64 -> 128x128\n",
        "    # x = layers.Conv2DTranspose(16, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # # 128x128 -> 256x256\n",
        "    # x = layers.Conv2DTranspose(3, 4, strides=2, padding='same')(x)\n",
        "    # x = layers.Activation('tanh')(x)\n",
        "\n",
        "    # x = layers.Conv2DTranspose(512, 4, strides=1, padding='valid')(inputs)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Directly to 8x8 -> 16x16 -> 32x32 -> 64x64 -> 128x128 -> 256x256\n",
        "    # Skipping some intermediate steps for faster training\n",
        "    x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, 4, strides=4, padding='same')(x)  # Note the stride of 4\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, 4, strides=4, padding='same')(x)  # Note the stride of 4\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(3, 4, strides=2, padding='same')(x)\n",
        "    x = layers.Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name=\"synthesis_network\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2b138e-3913-4229-ab08-f23356d86f7d",
      "metadata": {
        "id": "9f2b138e-3913-4229-ab08-f23356d86f7d"
      },
      "outputs": [],
      "source": [
        "class StyleGAN(Model):\n",
        "    def __init__(self, latent_dim=512, style_dim=512):\n",
        "        super(StyleGAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.style_dim = style_dim\n",
        "\n",
        "        self.mapping = build_style_mapping(latent_dim, style_dim)\n",
        "        self.age_embedding = build_age_embedding(style_dim)\n",
        "        self.synthesis = build_synthesis_network(style_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if isinstance(inputs, list) and len(inputs) == 2:\n",
        "            latent, age = inputs\n",
        "        else:\n",
        "            raise ValueError(\"StyleGAN expects inputs as [latent, age]\")\n",
        "\n",
        "        # Make sure age is a tensor with shape (batch_size, 1)\n",
        "        age = tf.expand_dims(age, axis=-1)\n",
        "\n",
        "        # Map latent to W space\n",
        "        w = self.mapping(latent)\n",
        "\n",
        "        # Generate age-specific style\n",
        "        age_style = self.age_embedding(age)\n",
        "\n",
        "        # Combine style and age\n",
        "        combined_style = w + age_style\n",
        "\n",
        "        # Reshape for synthesis\n",
        "        x = tf.reshape(combined_style, [-1, 1, 1, self.style_dim])\n",
        "\n",
        "        # Generate image\n",
        "        return self.synthesis(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e67fee-bb02-499f-9744-87137f16dc66",
      "metadata": {
        "id": "10e67fee-bb02-499f-9744-87137f16dc66"
      },
      "outputs": [],
      "source": [
        "class IdentityLoss(Model):\n",
        "    def __init__(self):\n",
        "        super(IdentityLoss, self).__init__()\n",
        "        base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet',\n",
        "                                                   input_shape=(128,128, 3))\n",
        "        self.identity_model = Model(inputs=base_model.input, outputs=base_model.output)\n",
        "        # Freeze the weights\n",
        "        self.identity_model.trainable = False\n",
        "\n",
        "    def call(self, real_images, generated_images):\n",
        "        real_features = self.identity_model(real_images)\n",
        "        gen_features = self.identity_model(generated_images)\n",
        "\n",
        "        # Flatten features before computing loss\n",
        "        real_features = layers.Flatten()(real_features)\n",
        "        gen_features = layers.Flatten()(gen_features)\n",
        "\n",
        "        # MSE loss between feature vectors\n",
        "        return tf.reduce_mean(tf.square(real_features - gen_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71189151-40da-46aa-aa3a-77769ee8569d",
      "metadata": {
        "id": "71189151-40da-46aa-aa3a-77769ee8569d"
      },
      "outputs": [],
      "source": [
        "class AgeManipulator:\n",
        "    def __init__(self, model_path=None):\n",
        "        self.latent_dim = 512\n",
        "        self.style_dim = 512\n",
        "\n",
        "        # Initialize models\n",
        "        self.generator = StyleGAN(self.latent_dim, self.style_dim)\n",
        "        self.discriminator = build_discriminator()\n",
        "        self.age_encoder = build_age_encoder(self.latent_dim)\n",
        "        self.age_classifier = build_age_classifier()\n",
        "        self.identity_loss = IdentityLoss()\n",
        "\n",
        "        # Compile models\n",
        "        self.g_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.d_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.e_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        self.c_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "        # Load weights if provided\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.load_weights(model_path)\n",
        "\n",
        "\n",
        "\n",
        "    def train_step(self, real_images, real_ages):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # Generate random target ages - ensure it's a 1D tensor\n",
        "        target_ages = tf.random.uniform(shape=(batch_size,), minval=20, maxval=80)\n",
        "\n",
        "        # Train discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            # Get real latent codes\n",
        "            latent_codes = self.age_encoder(real_images)\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_images = self.generator([latent_codes, target_ages])\n",
        "\n",
        "            # Discriminator predictions\n",
        "            real_preds = self.discriminator(real_images)\n",
        "            fake_preds = self.discriminator(fake_images)\n",
        "\n",
        "            # Discriminator losses\n",
        "            d_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
        "                tf.ones_like(real_preds), real_preds))\n",
        "            d_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
        "                tf.zeros_like(fake_preds), fake_preds))\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        # Apply discriminator gradients\n",
        "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Train generator, encoder, and age classifier\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Get real latent codes\n",
        "            latent_codes = self.age_encoder(real_images)\n",
        "\n",
        "            # Generate fake images with target ages\n",
        "            fake_images = self.generator([latent_codes, target_ages])\n",
        "\n",
        "            # Reconstruct original images\n",
        "            rec_images = self.generator([latent_codes, real_ages])\n",
        "\n",
        "            # Discriminator predictions for generated images\n",
        "            fake_preds = self.discriminator(fake_images)\n",
        "\n",
        "            # Adversarial loss\n",
        "            g_loss_adv = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
        "                tf.ones_like(fake_preds), fake_preds))\n",
        "\n",
        "            # Identity loss\n",
        "            id_loss = self.identity_loss(real_images, fake_images)\n",
        "\n",
        "            # Age classification loss - ensure we expand target_ages to match model output\n",
        "            pred_ages = self.age_classifier(fake_images)\n",
        "            target_ages_expanded = tf.expand_dims(target_ages, axis=-1)\n",
        "            age_class_loss = tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(target_ages_expanded, pred_ages))\n",
        "\n",
        "            # Reconstruction loss\n",
        "            rec_loss = tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(real_images, rec_images))\n",
        "\n",
        "            # Combined losses with weighting\n",
        "            g_loss = g_loss_adv + 10.0 * id_loss + 5.0 * age_class_loss + 15.0 * rec_loss\n",
        "\n",
        "        # Apply gradients\n",
        "        g_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        e_gradients = tape.gradient(g_loss, self.age_encoder.trainable_variables)\n",
        "        c_gradients = tape.gradient(age_class_loss, self.age_classifier.trainable_variables)\n",
        "\n",
        "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
        "        self.e_optimizer.apply_gradients(zip(e_gradients, self.age_encoder.trainable_variables))\n",
        "        self.c_optimizer.apply_gradients(zip(c_gradients, self.age_classifier.trainable_variables))\n",
        "\n",
        "        return {\n",
        "            \"d_loss\": d_loss,\n",
        "            \"g_loss\": g_loss,\n",
        "            \"id_loss\": id_loss,\n",
        "            \"age_loss\": age_class_loss,\n",
        "            \"rec_loss\": rec_loss\n",
        "        }\n",
        "\n",
        "    # def train(self, dataset, epochs=100):\n",
        "    #     for epoch in range(epochs):\n",
        "    #         print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    #         # Initialize metrics\n",
        "    #         metrics = {\n",
        "    #             \"d_loss\": [],\n",
        "    #             \"g_loss\": [],\n",
        "    #             \"id_loss\": [],\n",
        "    #             \"age_loss\": [],\n",
        "    #             \"rec_loss\": []\n",
        "    #         }\n",
        "\n",
        "    #         # Training loop\n",
        "    #         for batch_images, batch_ages in dataset:\n",
        "    #             batch_results = self.train_step(batch_images, batch_ages)\n",
        "\n",
        "    #             # Update metrics\n",
        "    #             for k, v in batch_results.items():\n",
        "    #                 metrics[k].append(v.numpy())\n",
        "\n",
        "    #         # Print epoch results\n",
        "    #         print(f\"D Loss: {np.mean(metrics['d_loss']):.4f}, G Loss: {np.mean(metrics['g_loss']):.4f}\")\n",
        "    #         print(f\"Identity Loss: {np.mean(metrics['id_loss']):.4f}, Age Loss: {np.mean(metrics['age_loss']):.4f}\")\n",
        "\n",
        "    #         # Save checkpoint every 10 epochs\n",
        "    #         if (epoch + 1) % 10 == 0:\n",
        "    #             self.save_weights(f\"checkpoint_epoch_{epoch+1}\")\n",
        "\n",
        "\n",
        "    def train(self, dataset, epochs=100):\n",
        "    # Get an estimate of the dataset size if possible\n",
        "        try:\n",
        "            dataset_size = len(list(dataset))\n",
        "            print(f\"Total batches per epoch: {dataset_size}\")\n",
        "        except:\n",
        "            dataset_size = None\n",
        "            print(\"Unable to determine dataset size\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Initialize metrics\n",
        "            metrics = {\n",
        "                \"d_loss\": [],\n",
        "                \"g_loss\": [],\n",
        "                \"id_loss\": [],\n",
        "                \"age_loss\": [],\n",
        "                \"rec_loss\": []\n",
        "            }\n",
        "\n",
        "            # Training loop with progress tracking\n",
        "            batch_count = 0\n",
        "            try:\n",
        "                from tqdm import tqdm\n",
        "                batch_iterator = tqdm(dataset, total=dataset_size)\n",
        "            except ImportError:\n",
        "                batch_iterator = dataset\n",
        "\n",
        "            for batch_images, batch_ages in batch_iterator:\n",
        "                batch_results = self.train_step(batch_images, batch_ages)\n",
        "                batch_count += 1\n",
        "\n",
        "                # Update metrics\n",
        "                for k, v in batch_results.items():\n",
        "                    metrics[k].append(v.numpy())\n",
        "\n",
        "                # Print progress every 10 batches\n",
        "                if batch_count % 10 == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    g_loss = np.mean(metrics['g_loss'][-10:])\n",
        "                    d_loss = np.mean(metrics['d_loss'][-10:])\n",
        "                    print(f\"  Batch {batch_count}: Time: {elapsed:.1f}s, G Loss: {g_loss:.4f}, D Loss: {d_loss:.4f}\")\n",
        "\n",
        "            # Print epoch results\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(f\"Epoch completed in {epoch_time:.1f}s\")\n",
        "            print(f\"D Loss: {np.mean(metrics['d_loss']):.4f}, G Loss: {np.mean(metrics['g_loss']):.4f}\")\n",
        "            print(f\"Identity Loss: {np.mean(metrics['id_loss']):.4f}, Age Loss: {np.mean(metrics['age_loss']):.4f}\")\n",
        "            print(f\"Reconstruction Loss: {np.mean(metrics['rec_loss']):.4f}\")\n",
        "\n",
        "            # Save checkpoint every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                self.save_weights(f\"checkpoint_epoch_{epoch+1}\")\n",
        "                print(f\"Checkpoint saved: checkpoint_epoch_{epoch+1}\")\n",
        "\n",
        "    # def manipulate_age(self, image_path, target_age):\n",
        "    #     \"\"\"\n",
        "    #     Manipulate the age of a person in an input image\n",
        "    #     Args:\n",
        "    #         image_path: Path to input image\n",
        "    #         target_age: Desired age for the output image\n",
        "    #     Returns:\n",
        "    #         Transformed image with the desired age\n",
        "    #     \"\"\"\n",
        "    #     # Load and preprocess image\n",
        "    #     image = tf.io.read_file(image_path)\n",
        "    #     image = tf.image.decode_jpeg(image, channels=3)\n",
        "    #     image = tf.image.resize(image, (256, 256))\n",
        "    #     image = (image / 127.5) - 1  # Normalize to [-1, 1]\n",
        "    #     image = tf.expand_dims(image, 0)  # Add batch dimension\n",
        "\n",
        "    #     # Convert target age to tensor - keep it as a 1D tensor\n",
        "    #     target_age = tf.constant([target_age], dtype=tf.float32)\n",
        "\n",
        "    #     # Encode input image to latent space\n",
        "    #     latent_code = self.age_encoder(image)\n",
        "\n",
        "    #     # Generate aged image\n",
        "    #     aged_image = self.generator([latent_code, target_age])\n",
        "\n",
        "    #     # Convert back to uint8 for display\n",
        "    #     aged_image = (aged_image[0] * 0.5 + 0.5) * 255.0\n",
        "    #     aged_image = tf.cast(aged_image, tf.uint8)\n",
        "\n",
        "    #     return aged_image\n",
        "\n",
        "\n",
        "    def manipulate_age(self, image_path, target_age):\n",
        "        \"\"\"\n",
        "        Manipulate the age of a person in an input image\n",
        "        Args:\n",
        "            image_path: Path to input image\n",
        "            target_age: Desired age for the output image\n",
        "        Returns:\n",
        "            Transformed image with the desired age\n",
        "        \"\"\"\n",
        "        # Load and preprocess image\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, (128,128))\n",
        "        image = tf.cast(image, tf.float32)  # Keep in [0, 255] range\n",
        "        image = tf.expand_dims(image, 0)  # Add batch dimension\n",
        "\n",
        "        # Convert target age to tensor - keep it as a 1D tensor\n",
        "        target_age = tf.constant([target_age], dtype=tf.float32)\n",
        "\n",
        "        # Encode input image to latent space\n",
        "        latent_code = self.age_encoder(image)\n",
        "\n",
        "        # Generate aged image\n",
        "        aged_image = self.generator([latent_code, target_age])\n",
        "\n",
        "        # Convert back to uint8 for display (output should already be in [0, 255] range)\n",
        "        aged_image = tf.cast(aged_image[0], tf.uint8)\n",
        "\n",
        "        return aged_image\n",
        "\n",
        "    def save_weights(self, path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        self.generator.save_weights(os.path.join(path, \"generator.h5\"))\n",
        "        self.discriminator.save_weights(os.path.join(path, \"discriminator.h5\"))\n",
        "        self.age_encoder.save_weights(os.path.join(path, \"age_encoder.h5\"))\n",
        "        self.age_classifier.save_weights(os.path.join(path, \"age_classifier.h5\"))\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        self.generator.load_weights(os.path.join(path, \"generator.h5\"))\n",
        "        self.discriminator.load_weights(os.path.join(path, \"discriminator.h5\"))\n",
        "        self.age_encoder.load_weights(os.path.join(path, \"age_encoder.h5\"))\n",
        "        self.age_classifier.load_weights(os.path.join(path, \"age_classifier.h5\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb11aff4-b96a-4f99-882a-7b3254729bef",
      "metadata": {
        "id": "cb11aff4-b96a-4f99-882a-7b3254729bef",
        "outputId": "b2f5c399-0b45-4ef8-ecf0-9b2e5c150fa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "manipulator = AgeManipulator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c75d0c8-ec8b-4029-b43b-76bfed2bf5fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c75d0c8-ec8b-4029-b43b-76bfed2bf5fb",
        "outputId": "251cb1db-4416-4882-a6a3-a6db4220adde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV loaded with 40012 entries\n",
            "Dataset reduced to 9994 entries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-4293687863.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df = df.groupby('age_group', group_keys=False).apply(\n",
            "/tmp/ipython-input-5-4293687863.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby('age_group', group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 99 valid paths among first 100 checked\n",
            "Creating dataset with 9994 images\n"
          ]
        }
      ],
      "source": [
        "dataset = MorphDataset(images_dir=\"/content/drive/MyDrive/genp1/Dataset/Images/Train\",csv_path=\"/content/drive/MyDrive/genp1/Dataset/Index/Train.csv\", batch_size=32, image_size=(128,128))\n",
        "train_ds = dataset.create_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baad129a-a62a-4536-9521-f8c9da6e362b",
      "metadata": {
        "id": "baad129a-a62a-4536-9521-f8c9da6e362b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4e2022-cf26-4317-d7af-1dbfc67e14d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches per epoch: 313\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 10/313 [00:52<12:13,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 10: Time: 52.1s, G Loss: 200.7029, D Loss: 0.8613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 20/313 [01:12<10:29,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 20: Time: 72.9s, G Loss: 63.5262, D Loss: 0.1882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 30/313 [01:32<09:13,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 30: Time: 92.8s, G Loss: 44.6965, D Loss: 0.0217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 40/313 [01:53<09:24,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 40: Time: 113.6s, G Loss: 44.8223, D Loss: 0.0057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 50/313 [02:13<08:43,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 50: Time: 134.0s, G Loss: 45.1892, D Loss: 0.0041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 60/313 [02:34<08:28,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 60: Time: 154.4s, G Loss: 48.3597, D Loss: 0.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 70/313 [02:55<08:33,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 70: Time: 175.0s, G Loss: 53.4104, D Loss: 0.0019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 80/313 [03:15<07:47,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 80: Time: 195.3s, G Loss: 36.7684, D Loss: 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 90/313 [03:36<07:58,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 90: Time: 216.2s, G Loss: 44.9417, D Loss: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 100/313 [03:56<06:56,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 100: Time: 236.0s, G Loss: 61.3531, D Loss: 0.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 110/313 [04:16<07:04,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 110: Time: 256.8s, G Loss: 47.8518, D Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 120/313 [04:36<06:17,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 120: Time: 276.8s, G Loss: 37.2760, D Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 130/313 [04:57<06:12,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 130: Time: 297.6s, G Loss: 41.2233, D Loss: 0.0058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 140/313 [05:17<05:53,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 140: Time: 317.8s, G Loss: 41.6263, D Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 150/313 [05:37<05:22,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 150: Time: 338.0s, G Loss: 34.8580, D Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 160/313 [05:58<05:32,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 160: Time: 358.8s, G Loss: 43.8078, D Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 170/313 [06:18<04:42,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 170: Time: 378.6s, G Loss: 57.7629, D Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 180/313 [06:39<04:40,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 180: Time: 399.3s, G Loss: 39.3608, D Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 190/313 [06:59<04:07,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 190: Time: 419.5s, G Loss: 36.9179, D Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 200/313 [07:20<03:51,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 200: Time: 440.1s, G Loss: 56.1990, D Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 210/313 [07:39<03:20,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 210: Time: 460.0s, G Loss: 59.9393, D Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 220/313 [08:00<03:07,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 220: Time: 480.6s, G Loss: 40.7942, D Loss: 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 230/313 [08:21<02:59,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 230: Time: 501.4s, G Loss: 34.6721, D Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 240/313 [08:41<02:25,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 240: Time: 521.4s, G Loss: 38.4440, D Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 250/313 [09:02<02:14,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 250: Time: 542.2s, G Loss: 37.0417, D Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 260/313 [09:22<01:44,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 260: Time: 562.1s, G Loss: 49.4806, D Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▋ | 270/313 [09:43<01:30,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 270: Time: 583.2s, G Loss: 51.1158, D Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 280/313 [10:03<01:04,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 280: Time: 603.1s, G Loss: 38.8091, D Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 290/313 [10:23<00:46,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 290: Time: 623.9s, G Loss: 73.3636, D Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 300/313 [10:44<00:26,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 300: Time: 644.2s, G Loss: 54.1470, D Loss: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 310/313 [11:04<00:05,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 310: Time: 664.6s, G Loss: 41.7782, D Loss: 0.3123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [11:21<00:00,  2.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch completed in 681.9s\n",
            "D Loss: 0.0555, G Loss: 51.6770\n",
            "Identity Loss: 0.0333, Age Loss: 7.2116\n",
            "Reconstruction Loss: 0.4144\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 10/313 [00:44<12:09,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 10: Time: 44.1s, G Loss: 58.1749, D Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 20/313 [01:04<09:34,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 20: Time: 64.0s, G Loss: 41.7626, D Loss: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 30/313 [01:24<09:40,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 30: Time: 84.9s, G Loss: 38.7595, D Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 40/313 [01:44<09:03,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 40: Time: 105.0s, G Loss: 44.4631, D Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 50/313 [02:05<08:46,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 50: Time: 125.5s, G Loss: 53.5828, D Loss: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 60/313 [02:26<09:08,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 60: Time: 146.1s, G Loss: 71.5200, D Loss: 0.0506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 70/313 [02:46<08:02,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 70: Time: 166.2s, G Loss: 110.5604, D Loss: 0.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 80/313 [03:06<08:15,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 80: Time: 187.0s, G Loss: 82.9164, D Loss: 0.0135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 90/313 [03:26<07:15,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 90: Time: 206.8s, G Loss: 91.8356, D Loss: 0.0224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 100/313 [03:47<07:22,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 100: Time: 227.6s, G Loss: 79.5139, D Loss: 0.0148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 110/313 [04:07<06:33,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 110: Time: 247.6s, G Loss: 56.8593, D Loss: 1.6786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 120/313 [04:28<06:29,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 120: Time: 268.3s, G Loss: 46.5115, D Loss: 0.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 130/313 [04:48<06:23,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 130: Time: 288.7s, G Loss: 37.7099, D Loss: 0.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 140/313 [05:09<05:44,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 140: Time: 309.0s, G Loss: 36.7563, D Loss: 0.0086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 150/313 [05:30<06:04,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 150: Time: 331.0s, G Loss: 29.3992, D Loss: 0.0097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 160/313 [05:51<05:14,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 160: Time: 351.6s, G Loss: 37.9539, D Loss: 0.1058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 170/313 [06:14<05:30,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 170: Time: 374.3s, G Loss: 33.1651, D Loss: 0.0237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 180/313 [06:36<04:57,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 180: Time: 396.5s, G Loss: 31.4320, D Loss: 0.0195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 190/313 [06:56<04:05,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 190: Time: 416.8s, G Loss: 30.4454, D Loss: 0.0115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 200/313 [07:17<04:01,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 200: Time: 437.9s, G Loss: 33.2359, D Loss: 0.0155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 210/313 [07:38<03:37,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 210: Time: 458.7s, G Loss: 48.8712, D Loss: 0.3620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 220/313 [07:59<03:11,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 220: Time: 479.8s, G Loss: 39.0237, D Loss: 0.0034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 230/313 [08:20<03:01,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 230: Time: 500.8s, G Loss: 39.9513, D Loss: 0.1267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 240/313 [08:41<02:27,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 240: Time: 521.1s, G Loss: 42.1328, D Loss: 0.0646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 250/313 [09:02<02:12,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 250: Time: 542.4s, G Loss: 29.8472, D Loss: 0.0028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 260/313 [09:22<01:44,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 260: Time: 562.5s, G Loss: 33.6171, D Loss: 0.0712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▋ | 270/313 [09:43<01:28,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 270: Time: 583.6s, G Loss: 43.0255, D Loss: 0.0617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 280/313 [10:04<01:11,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 280: Time: 604.5s, G Loss: 45.9512, D Loss: 0.1017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 290/313 [10:24<00:45,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 290: Time: 624.9s, G Loss: 47.2996, D Loss: 0.0288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 300/313 [10:45<00:28,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 300: Time: 645.8s, G Loss: 42.1543, D Loss: 0.1690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 310/313 [11:06<00:05,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 310: Time: 666.1s, G Loss: 48.8577, D Loss: 0.0554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [11:12<00:00,  2.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch completed in 672.7s\n",
            "D Loss: 0.0973, G Loss: 48.4408\n",
            "Identity Loss: 0.0313, Age Loss: 4.4970\n",
            "Reconstruction Loss: 0.4152\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/313 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        " manipulator.train(train_ds, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182546a3-c552-4625-ab3f-f8a9e4a7c178",
      "metadata": {
        "id": "182546a3-c552-4625-ab3f-f8a9e4a7c178"
      },
      "outputs": [],
      "source": [
        "result = manipulator.manipulate_age(\"/content/test.jpeg\", 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074f43b3-0ce1-4309-be14-cb92494debf5",
      "metadata": {
        "id": "074f43b3-0ce1-4309-be14-cb92494debf5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"original\")\n",
        "plt.imshow(Image.open(\"/content/test.jpeg\"))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"manipulated\")\n",
        "plt.imshow(result.numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JSTA14O5Qxc"
      },
      "id": "9JSTA14O5Qxc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}